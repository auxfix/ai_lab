# RAG System Visual Architecture Guide
## Detailed Diagrams and Flowcharts

---

## 1. High-Level System Architecture

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    YOUR RAG SYSTEM                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INDEXING PIPELINE (Run once or when code changes)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ“ Codebase                     ğŸ§  Models
    â”œâ”€ auth.py                      â”œâ”€ SentenceTransformer
    â”œâ”€ api.py                       â”‚  (all-mpnet-base-v2)
    â”œâ”€ models.py                    â”‚  768 dimensions
    â””â”€ utils.py                     â”‚  On GPU: RTX 3090
         â”‚                          â”‚
         â†“                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚ CodeMiner  â”‚                 â”‚
    â”‚ (Loader)   â”‚                 â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â”‚
          â”‚                        â”‚
          â†“                        â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚ Raw Code     â”‚               â”‚
    â”‚ Files List   â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
           â”‚                       â”‚
           â†“                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
    â”‚SmartChunker  â”‚               â”‚
    â”‚(Text Splitter)â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
           â”‚                       â”‚
           â†“                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚ Code Chunks  â”‚               â”‚
    â”‚ ~1500 tokens â”‚               â”‚
    â”‚ each         â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
           â”‚                       â”‚
           â†“                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Vectorizer   â”‚â—„â”€â”€â”€â”€â”€â”¤ Embedding Model â”‚
    â”‚ (Encoder)    â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              
           â”‚                       
           â†“                       
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               
    â”‚ Embeddings   â”‚               
    â”‚ [0.2, 0.3,   â”‚               
    â”‚  ..., 0.7]   â”‚               
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               
           â”‚                       
           â†“                       
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               
    â”‚  ChromaDB    â”‚               
    â”‚ (Vector DB)  â”‚               
    â”‚  Persistent  â”‚               
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  QUERY PIPELINE (Every user question)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ğŸ‘¤ User
     â”‚
     â†“
    "How does authentication work?"
     â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                                               â”‚
     â†“                                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚ Query Encoder  â”‚                                  â”‚
â”‚ (Same model)   â”‚                                  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
     â”‚                                               â”‚
     â†“                                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚ Query Vector   â”‚                                  â”‚
â”‚ [0.21, 0.43,   â”‚                                  â”‚
â”‚  ..., 0.67]    â”‚                                  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
     â”‚                                               â”‚
     â†“                                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚  Similarity    â”‚                                  â”‚
â”‚  Search        â”‚                                  â”‚
â”‚  (ChromaDB)    â”‚                                  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
     â”‚                                               â”‚
     â†“                                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚ Top K Chunks   â”‚                                  â”‚
â”‚ 1. auth.py:12  â”‚                                  â”‚
â”‚    (sim: 0.89) â”‚                                  â”‚
â”‚ 2. login.py:45 â”‚                                  â”‚
â”‚    (sim: 0.82) â”‚                                  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
     â”‚                                               â”‚
     â†“                                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚ Context Builder    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ (Prompt Formatter) â”‚  + Original Question
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "Context: <retrieved code>              â”‚
â”‚  Question: How does authentication work?â”‚
â”‚  Answer:"                               â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM          â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  CodeLlama 34B  â”‚
â”‚ (Generator)    â”‚         â”‚  On GPU         â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "Authentication in this system works   â”‚
â”‚  by first validating the user          â”‚
â”‚  credentials in the login() function..." â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
    ğŸ‘¤ User sees answer + source references
```

---

## 2. Embedding Process Detail

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              HOW TEXT BECOMES VECTORS                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Input Text: "def login(username, password):"

Step 1: TOKENIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"def login(username, password):"
         â†“
[101, 9355, 7712, 1006, 11224, 1010, 8385, 1007, 1024, 102]
  â†‘    â†‘     â†‘      â†‘     â†‘      â†‘     â†‘     â†‘     â†‘     â†‘
[CLS] def  login   (  username  ,  password  )    :   [SEP]

Special tokens:
[CLS] = 101  (Start of sequence)
[SEP] = 102  (End of sequence)

Step 2: TOKEN EMBEDDINGS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Each token ID â†’ Initial embedding vector (768 dims)

Token "def" (9355):
  Look up in embedding table:
  â†’ [0.023, -0.145, 0.089, ..., 0.234]  (768 numbers)

Token "login" (7712):
  â†’ [0.167, 0.023, -0.056, ..., 0.123]  (768 numbers)

Result: Sequence of 10 vectors (one per token)

Step 3: POSITIONAL ENCODING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Add position information (tokens need to know their order)

Position 0 (CLS):  [0.000, 1.000, 0.000, ...]
Position 1 (def):  [0.841, 0.540, 0.008, ...]
Position 2 (login):[0.909, -0.416, 0.032, ...]
...

Add to token embeddings:
Token embedding + Position embedding = Input to transformer

Step 4: TRANSFORMER LAYERS (12 layers)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Layer 1                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   MULTI-HEAD ATTENTION         â”‚  â”‚
â”‚  â”‚   (8 heads, each 96 dims)      â”‚  â”‚
â”‚  â”‚                                â”‚  â”‚
â”‚  â”‚   Head 1: Syntax patterns     â”‚  â”‚
â”‚  â”‚   Head 2: Semantic relations  â”‚  â”‚
â”‚  â”‚   ...                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â†“                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   FEED FORWARD                 â”‚  â”‚
â”‚  â”‚   768 â†’ 3072 â†’ 768             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â†“                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   LAYER NORMALIZATION          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Layer 2                      â”‚
â”‚         (same structure)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
                 ...
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Layer 12                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 5: POOLING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Combine all token vectors into one sentence vector

Methods:
1. CLS token (use first token's final state)
2. Mean pooling (average all tokens)  â† Your model uses this
3. Max pooling (take maximum values)

Mean Pooling:
Token 1: [0.2, 0.3, 0.1, ...]
Token 2: [0.4, 0.1, 0.3, ...]
Token 3: [0.1, 0.5, 0.2, ...]
         â†“ Average â†“
Result:  [0.233, 0.3, 0.2, ...]  (768 dims)

Step 6: NORMALIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Normalize vector to unit length (for cosine similarity)

Vector: [0.233, 0.3, 0.2, ..., 0.15]
Length: âˆš(0.233Â² + 0.3Â² + 0.2Â² + ... + 0.15Â²) = 1.456

Normalized: [0.16, 0.206, 0.137, ..., 0.103]
            â†‘_________________________________â†‘
            Each value divided by 1.456
            New length = 1.0

FINAL OUTPUT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"def login(username, password):"
         â†“
[0.16, 0.206, 0.137, ..., 0.103]  (768 numbers)
 â†‘______________________________â†‘
 This represents the MEANING
```

---

## 3. Attention Mechanism Visualization

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              SELF-ATTENTION EXPLAINED                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Input: "def login ( username ) :"

Each word "looks at" other words:

ATTENTION SCORES (what each word focuses on):

       def   login   (   username   )    :
def    1.0   0.3    0.0    0.1     0.0  0.0   â† "def" mostly looks at itself
login  0.4   1.0    0.2    0.3     0.1  0.0   â† "login" looks at "def" & "username"
(      0.0   0.3    1.0    0.2     0.5  0.0   â† "(" looks at "login" & "username"
username 0.1 0.4    0.2    1.0     0.2  0.1   â† "username" looks at "login"
)      0.0   0.1    0.5    0.2     1.0  0.2   â† ")" looks at "("
:      0.0   0.2    0.0    0.1     0.1  1.0   â† ":" looks at nearby tokens

Higher score = stronger relationship

VISUAL REPRESENTATION:

    def â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“                  â†“
   login â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ username
     â†‘                  â†‘
     â”‚                  â”‚
     (  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ )
                        â†“
                        :

Thick lines = strong attention
Thin lines = weak attention

WHY THIS MATTERS:

The model learns:
- "def" introduces functions
- "login" is the function name
- "username" is a parameter
- Parentheses group parameters
- ":" ends the signature

This contextual understanding is encoded in the embedding!

MULTI-HEAD ATTENTION:

Different heads learn different patterns:

Head 1 (Syntax):
  def â†’ login (strong)
  ( â†’ ) (strong)

Head 2 (Semantics):
  login â†’ username (strong)
  (understands "login needs username")

Head 3 (Structure):
  def â†’ : (strong)
  (understands "def...:" pattern)

All heads combine to form rich understanding!
```

---

## 4. Vector Similarity Search

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           HOW CHROMADB FINDS SIMILAR CODE                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

VECTOR SPACE (Simplified to 2D for visualization)

Actual: 768 dimensions (impossible to visualize!)

         Authentication Code
              Region
               â†“
    â”‚     â€¢ chunk: "def login()"
    â”‚    â€¢  chunk: "check_password()"
    â”‚   â€¢   chunk: "session.create()"
    â”‚
    â”‚
    â”‚                           â€¢ chunk: "import numpy"
    â”‚                          â€¢  chunk: "calculate_mean()"
    â”‚                         â€¢   API/Math Code Region
    â”‚
    â”‚  â€¢ Query: "how to authenticate?"
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’

COSINE SIMILARITY:

Vector A: Query "how to authenticate?"
Vector B: Chunk "def login()"

         B
        /â”‚
       / â”‚
      /  â”‚
     /   â”‚ Angle Î¸
    /    â”‚
   /_____|
  A

Similarity = cos(Î¸)

Î¸ = 15Â° â†’ cos(15Â°) = 0.97  (very similar!)
Î¸ = 45Â° â†’ cos(45Â°) = 0.71  (somewhat similar)
Î¸ = 90Â° â†’ cos(90Â°) = 0.0   (unrelated)

HNSW SEARCH ALGORITHM:

Think of it as a highway system:

Level 2 (Express):   â€¢â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â€¢
                    /                  \
                   /                    \
Level 1 (Road):   â€¢â”€â”€â”€â€¢â”€â”€â”€â”€â”€â”€â”€â€¢â”€â”€â”€â”€â”€â€¢â”€â”€â”€â€¢
                 / \  â”‚  â•± â•²  â”‚  â•±  â”‚ â•² â”‚
Level 0 (Local): â€¢â”€â€¢â”€â€¢â”€â€¢â”€â”€â€¢â”€â€¢â”€â”€â€¢â”€â”€â€¢â”€â€¢â”€â€¢â”€â€¢
                       â†‘
                    Start here

Search Process:
1. Start at random node in Level 2
2. Jump to closest neighbor
3. Drop to Level 1
4. Find closest neighbors at this level
5. Drop to Level 0
6. Refine search locally
7. Return K nearest neighbors

Complexity: O(log N) instead of O(N)!

Example with 100,000 chunks:
- Brute force: Check all 100,000 â†’ 100,000 comparisons
- HNSW: Check ~logâ‚‚(100,000) â‰ˆ 17 levels â†’ ~1,000 comparisons

100x faster! âš¡

YOUR QUERY FLOW:

1. Query: "how to authenticate?"
   â†’ Embed: [0.21, 0.43, ..., 0.67]

2. HNSW Search in ChromaDB:
   - Start at top level
   - Navigate to neighborhood
   - Check ~1000 candidates (not all 100K!)

3. Results (sorted by similarity):
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 1. auth.py:12    Similarity: 0.89  â”‚ â† Best match
   â”‚    "def login(user, pwd):"         â”‚
   â”‚                                    â”‚
   â”‚ 2. session.py:45  Similarity: 0.82 â”‚
   â”‚    "def create_session(uid):"      â”‚
   â”‚                                    â”‚
   â”‚ 3. auth.py:67    Similarity: 0.78  â”‚
   â”‚    "def check_credentials():"      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. Return top K (your setting: K=8)
```

---

## 5. LLM Generation Process

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        HOW CODELLAMA GENERATES ANSWERS                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AUTOREGRESSIVE GENERATION (token by token)

Input Prompt:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context: [retrieved code chunks]           â”‚
â”‚ Question: How does authentication work?    â”‚
â”‚ Answer:                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Generate first token
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Prompt â†’ LLM â†’ Probability distribution over all tokens

Top predictions:
"Authentication" : 0.45  â† Pick this (highest)
"The"           : 0.23
"In"            : 0.15
"Based"         : 0.08
...

New prompt:
"... Answer: Authentication"

Step 2: Generate second token
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"... Answer: Authentication" â†’ LLM â†’ Probabilities

Top predictions:
"in"      : 0.35  â† Pick this
"works"   : 0.28
"is"      : 0.20
...

New prompt:
"... Answer: Authentication in"

Step 3: Continue until done
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"... Answer: Authentication in this" â†’ LLM â†’ ...
"... Answer: Authentication in this system" â†’ LLM â†’ ...
"... Answer: Authentication in this system works" â†’ LLM â†’ ...

Stop when:
- Generate [END] token, OR
- Reach max length (your setting: 1000 tokens), OR
- User interrupts

TEMPERATURE EFFECT:

Temperature = 0.0 (Deterministic):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Always pick highest    â”‚
â”‚ "Authentication" (45%) â”‚ â† Always this
â”‚ "The" (23%)           â”‚
â”‚ "In" (15%)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Temperature = 0.7 (Balanced, your setting):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample from top tokens â”‚
â”‚ "Authentication" (45%) â”‚ â† Usually this
â”‚ "The" (23%)           â”‚ â† Sometimes this
â”‚ "In" (15%)            â”‚ â† Rarely this
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Temperature = 1.5 (Creative):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sample broadly         â”‚
â”‚ "Authentication" (45%) â”‚ â† Often
â”‚ "The" (23%)           â”‚ â† Often
â”‚ "In" (15%)            â”‚ â† Sometimes
â”‚ "Code" (2%)           â”‚ â† Even low prob tokens
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KV-CACHE OPTIMIZATION:

Without cache:
Step 1: Process "Answer: Authentication"
Step 2: Process "Answer: Authentication in"         â† Reprocess everything!
Step 3: Process "Answer: Authentication in this"    â† Reprocess everything!
        â†‘_______________________________________â†‘
        Wasteful! Recomputing same tokens

With cache (your LLM uses this):
Step 1: Process "Answer: Authentication"
        Cache keys & values for "Answer:" and "Authentication"
Step 2: Process only "in" (reuse cached KV)         â† Much faster!
Step 3: Process only "this" (reuse cached KV)       â† Much faster!

Result: ~5-10x faster generation!

YOUR GPU DURING GENERATION:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RTX 3090 VRAM (24GB)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚ Model Weights:     19GB (CodeLlama 34B)â”‚ â† Static
â”‚ KV Cache:          2-3GB                â”‚ â† Grows with length
â”‚ Activation:        1GB                  â”‚ â† Changes each token
â”‚ Embedding Model:   1.5GB                â”‚ â† Static
â”‚ Free:              0.5-1.5GB            â”‚ â† Buffer
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Tensor Cores in action:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Matrix Multiplication  â”‚ â† Where Tensor Cores shine
â”‚ (most of LLM compute)  â”‚
â”‚                        â”‚
â”‚ Speed: ~30 TFLOPS      â”‚ â† Your 3090
â”‚ = 30 trillion ops/sec  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Training Process (How Models Are Made)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           MODEL TRAINING (You don't do this!)                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EMBEDDING MODEL TRAINING (all-mpnet-base-v2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Phase 1: MASKED LANGUAGE MODELING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input:  "The quick brown [MASK] jumps"
Target: "fox"

Model learns: Context â†’ Predict masked word

Training Data: 1 billion sentences
Time: 1-2 weeks on 8x V100 GPUs
Cost: ~$100K

Phase 2: CONTRASTIVE LEARNING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Positive pairs (should be close):
"def login(user):" â†â†’ "user login function"
    Embedding A          Embedding B
    â”‚                    â”‚
    â””â”€â”€â”€â”€â”€â”€ minimize â”€â”€â”€â”€â”˜
         distance

Negative pairs (should be far):
"def login(user):" â†â†’ "import numpy"
    Embedding A          Embedding C
    â”‚                    â”‚
    â””â”€â”€â”€â”€â”€â”€ maximize â”€â”€â”€â”€â”˜
         distance

Loss Function:
Loss = distance(A, B) - distance(A, C) + margin

Training Data: 1 billion pairs
Time: 1-2 weeks on 8x A100 GPUs
Cost: ~$200K

LLM TRAINING (CodeLlama 34B)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Phase 1: PRE-TRAINING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input:  "def fibonacci(n):\n    if n <= 1:\n        return"
Target: " n"

Next token:  " n"
Next target: "\n"
Next token:  "\n"
Next target: "    return"
... continues ...

Training Data: 5 trillion tokens
- GitHub code: 500B tokens
- Books: 100B tokens
- Wikipedia: 50B tokens
- Web: 4.35T tokens

Training Time: 2-3 months
Hardware: 2048 A100 GPUs (80GB each)
Power: 10 megawatts (small power plant!)
Cost: $20-30 million

Phase 2: INSTRUCTION TUNING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input:  "Write a function to reverse a string"
Target: "def reverse_string(s):\n    return s[::-1]"

Training Data: ~100K instruction pairs
Time: 1-2 weeks
Hardware: 256 A100 GPUs
Cost: ~$500K

Phase 3: RLHF (Reinforcement Learning from Human Feedback)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Generate multiple answers:
   Q: "Write a function to reverse a string"
   
   A1: "def reverse_string(s): return s[::-1]"
   A2: "def reverse_string(s):\n    return ''.join(reversed(s))"
   A3: "def rev(s): return s[-1::-1]"

2. Humans rank:
   Rank 1: A1 (clear and concise)
   Rank 2: A2 (verbose but correct)
   Rank 3: A3 (unclear name)

3. Train reward model:
   Learns to predict human preferences

4. Use reward model to fine-tune LLM:
   Generate â†’ Get reward â†’ Adjust weights

Training Data: ~10K ranked examples
Time: 1 week
Hardware: 128 A100 GPUs
Cost: ~$200K

TOTAL CODELLAMA TRAINING COST: ~$30 million

YOU DON'T NEED TO DO ANY OF THIS!
You just download and use the finished model! ğŸ‰
```

---

## 7. Your Complete System Data Flow

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              END-TO-END DATA TRANSFORMATION                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

START: Raw Code File
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ auth.py (10,000 lines, 300KB)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ import hashlib                       â”‚
â”‚ import jwt                           â”‚
â”‚ from database import User            â”‚
â”‚                                      â”‚
â”‚ def login(username, password):       â”‚
â”‚     """Authenticate user"""          â”‚
â”‚     user = User.query.filter_by(     â”‚
â”‚         username=username).first()   â”‚
â”‚     if user and check_password(...): â”‚
â”‚         return create_session(user)  â”‚
â”‚     return None                      â”‚
â”‚ ...                                  â”‚
â”‚ (9,950 more lines)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†“ CodeMiner.mine_all()

STEP 1: File Metadata
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{
  "path": "auth.py",
  "content": "import hashlib\nimport jwt...",
  "size": 300000,
  "language": "py"
}

â†“ SmartCodeChunker.chunk_with_context()

STEP 2: Split into Chunks
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Chunk 1:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ import hashlib                       â”‚
â”‚ import jwt                           â”‚
â”‚ from database import User            â”‚
â”‚                                      â”‚
â”‚ def login(username, password):       â”‚
â”‚     """Authenticate user"""          â”‚
â”‚     user = User.query.filter_by(     â”‚
â”‚         username=username).first()   â”‚
â”‚     if user and check_password(...): â”‚
â”‚         return create_session(user)  â”‚
â”‚     return None                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Metadata: {
  "source_file": "auth.py",
  "language": "py",
  "chunk_id": 0,
  "total_chunks": 20
}

Chunk 2: (with 200-token overlap from Chunk 1)
... continues ...

Total: 20 chunks from this file

â†“ CodeVectorizer.embed_and_store()

STEP 3: Create Embeddings
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Chunk 1 â†’ SentenceTransformer (on GPU) â†’
[0.234, -0.123, 0.567, 0.089, -0.234, ..., 0.456]
 â†‘________________________________________________â†‘
 768 floating-point numbers
 ~3KB per embedding

GPU Process:
- Tokenize text
- 12 transformer layers
- Self-attention computations
- Mean pooling
- Normalization
Time: ~2ms per chunk (GPU accelerated!)

â†“ ChromaDB.upsert()

STEP 4: Store in Vector Database
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ChromaDB Structure:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID: "auth.py_0_abc12345"             â”‚
â”‚ Vector: [0.234, -0.123, ..., 0.456] â”‚
â”‚ Document: "import hashlib\n..."      â”‚
â”‚ Metadata: {                          â”‚
â”‚   "source_file": "auth.py",          â”‚
â”‚   "language": "py",                  â”‚
â”‚   "chunk_id": 0                      â”‚
â”‚ }                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
... 19 more chunks from auth.py
... thousands more from other files

Stored on disk: ./chroma_db/
- Vectors: pickle format (~100MB per 10K chunks)
- Metadata: SQLite database
- Index: HNSW graph structure

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

QUERY TIME!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User Question: "How does authentication work?"

â†“ SentenceTransformer.encode()

STEP 5: Embed Query
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"How does authentication work?"
       â†“ (Same embedding model)
[0.212, -0.098, 0.523, 0.104, -0.211, ..., 0.432]
       â†‘___________________________________________â†‘
       768 numbers representing query meaning

â†“ ChromaDB.query()

STEP 6: Similarity Search
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Compare query vector with all stored vectors:

Query:  [0.212, -0.098, ..., 0.432]
Chunk1: [0.234, -0.123, ..., 0.456]  â†’ sim: 0.89 âœ“
Chunk2: [0.001,  0.876, ..., 0.123]  â†’ sim: 0.23 âœ—
Chunk3: [0.198, -0.087, ..., 0.445]  â†’ sim: 0.82 âœ“
...
Chunk1000: [0.223, -0.101, ..., 0.439] â†’ sim: 0.78 âœ“

HNSW algorithm: Only checks ~1000 chunks (not all 100K!)

Top 8 Results:
1. auth.py:0     (0.89) â† login function
2. session.py:5  (0.82) â† create_session
3. auth.py:3     (0.78) â† check_password
4. middleware.py (0.75) â† auth middleware
5. api.py:12     (0.72) â† login endpoint
6. models.py:8   (0.68) â† User model
7. utils.py:45   (0.65) â† hash_password
8. config.py:23  (0.62) â† JWT settings

â†“ CodeQueryEngine._format_context()

STEP 7: Format Prompt
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ You are a code assistant.             â”‚
â”‚                                        â”‚
â”‚ Context:                               â”‚
â”‚ ### Code Snippet 1 (from auth.py):    â”‚
â”‚ ```python                              â”‚
â”‚ def login(username, password):         â”‚
â”‚     user = User.query.filter_by(...    â”‚
â”‚     if user and check_password(...):   â”‚
â”‚         return create_session(user)    â”‚
â”‚     return None                        â”‚
â”‚ ```                                    â”‚
â”‚                                        â”‚
â”‚ ### Code Snippet 2 (from session.py): â”‚
â”‚ ```python                              â”‚
â”‚ def create_session(user):              â”‚
â”‚     token = jwt.encode({...            â”‚
â”‚     return token                       â”‚
â”‚ ```                                    â”‚
â”‚ ... (6 more snippets)                  â”‚
â”‚                                        â”‚
â”‚ Question: How does authentication work?â”‚
â”‚                                        â”‚
â”‚ Answer:                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Size: ~6000 tokens (fits in 8K context!)

â†“ ollama.generate()

STEP 8: LLM Generation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Prompt â†’ CodeLlama 34B (on GPU) â†’

Generation (token by token):
"Authentication" (3ms)
" in" (3ms)
" this" (3ms)
" system" (3ms)
... continues for ~200 tokens ...
... ~5 seconds total ...

Final Answer:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Authentication in this system works by â”‚
â”‚ using the login() function in auth.py. â”‚
â”‚ It takes a username and password,      â”‚
â”‚ queries the User database, validates   â”‚
â”‚ credentials with check_password(), and â”‚
â”‚ if valid, creates a JWT session using  â”‚
â”‚ create_session(). The session token is â”‚
â”‚ returned and used for subsequent       â”‚
â”‚ authenticated requests.                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†“ Display to user

STEP 9: Show Answer with Sources
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¤– Answer:                             â”‚
â”‚ Authentication in this system works... â”‚
â”‚                                        â”‚
â”‚ ğŸ“š Sources:                            â”‚
â”‚ 1. auth.py (similarity: 0.89)          â”‚
â”‚    Preview: def login(username...)     â”‚
â”‚ 2. session.py (similarity: 0.82)       â”‚
â”‚    Preview: def create_session...      â”‚
â”‚ ...                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

END: User gets answer in ~5-6 seconds! âœ…
```

---

## Summary

This visual guide shows:
1. âœ… Complete system architecture
2. âœ… How text becomes embeddings
3. âœ… How attention works
4. âœ… How vector search finds matches
5. âœ… How LLMs generate text
6. âœ… How models are trained (not by you!)
7. âœ… End-to-end data flow

You now understand the complete pipeline from code file to answer! ğŸ“

